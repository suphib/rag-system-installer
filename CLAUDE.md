# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a complete RAG (Retrieval Augmented Generation) system installation package for deploying on Debian 12 servers. The system combines:
- **Ollama** - LLM runtime for running Llama 3.1 models
- **Qdrant** - Vector database for semantic search
- **TypeScript API** - REST API for PDF upload, indexing, and chat functionality

Target deployment: EPYC 03 servers (24fire.de) with 4 cores, 24 GB RAM, 150 GB NVMe SSD, 3 TB monthly traffic.

**Example Server Configuration:**
- IPv4: 45.92.217.15 / Gateway: 45.92.216.1
- IPv6: 2a0a:51c4:5:677f:: / Gateway: 2a0a:51c4:5::1
- Resources: 4 cores, 24 GB RAM, 150 GB storage, 3 TB traffic/month

## Installation Commands

The project consists of sequential installation scripts:

```bash
# Master installation (runs all steps)
./00-install-all.sh

# Individual steps
./01-initial-setup.sh     # System setup, Docker, firewall, security
./02-install-rag.sh       # Qdrant + Ollama + Llama 3.1 8B model
./03-install-api.sh       # Node.js 20 + PM2 + dependencies
./04-create-api-code.sh   # Generate TypeScript API code + build
./05-test-system.sh       # System verification tests
```

All scripts must be made executable: `chmod +x *.sh`

Installation takes approximately 30-40 minutes (includes ~4.7 GB Llama model download).

## System Architecture

### Three-Tier Architecture

1. **LLM Layer (Ollama)**
   - Runs on port 11434
   - Handles text generation and embedding generation
   - Model: Llama 3.1 8B (configurable to 13B/70B with more RAM)
   - CPU-only inference (~5-8 tokens/sec on EPYC)

2. **Vector Database (Qdrant)**
   - Runs in Docker container
   - API port: 6333, UI port: 6335
   - Collection: "documents" (384-dim vectors)
   - Stores document chunks with metadata (filename, chunk index, text)

3. **API Layer (TypeScript/Express)**
   - REST API on port 3000
   - Managed by PM2 process manager
   - Handles PDF parsing, chunking, indexing, and RAG queries

### RAG Pipeline

**Indexing Flow:**
1. PDF uploaded via `/api/upload`
2. PDF parsed to extract text (pdf-parse library)
3. Text split into ~1000 character chunks (paragraph-aware)
4. Each chunk → Ollama embedding → Qdrant storage with metadata

**Query Flow:**
1. User question received at `/api/chat`
2. Question → Ollama embedding
3. Qdrant similarity search (default: top 5 chunks)
4. Relevant chunks assembled as context
5. Context + question → Ollama for answer generation
6. Return answer + source documents + processing time

### File Structure

```
/opt/rag-system/
├── docker-compose.yml       # Qdrant container config
├── qdrant/                  # Qdrant persistent storage (CRITICAL TO BACKUP)
├── documents/               # Optional PDF storage
├── logs/                    # API logs (PM2)
└── api/
    ├── src/
    │   ├── index.ts         # Express app entry point
    │   ├── routes/          # API endpoint definitions
    │   ├── services/
    │   │   ├── ollama.service.ts    # Ollama client wrapper
    │   │   ├── qdrant.service.ts    # Qdrant client wrapper
    │   │   └── rag.service.ts       # Core RAG logic
    │   └── types/           # TypeScript interfaces
    ├── dist/                # Compiled JavaScript
    ├── package.json
    ├── tsconfig.json
    ├── .env                 # Environment configuration
    └── ecosystem.config.js  # PM2 configuration
```

## Key Management Commands

### API Management (PM2)
```bash
pm2 status                  # View all services
pm2 logs rag-api            # Live logs
pm2 restart rag-api         # Restart API
pm2 stop rag-api            # Stop API
pm2 monit                   # Resource monitoring
```

### Qdrant Management
```bash
docker ps                   # Container status
docker logs qdrant          # View logs
docker restart qdrant       # Restart container
cd /opt/rag-system && docker-compose up -d    # Start all
```

### Ollama Management
```bash
ollama list                 # List installed models
ollama pull llama3.1:13b    # Download different model
ollama run llama3.1:8b      # Interactive test
systemctl status ollama     # Service status
```

## API Endpoints

**Base URL:** `http://SERVER-IP:3000`

- `GET /api/health` - Health check
- `POST /api/upload` - Upload PDF (multipart/form-data, field: "file")
- `POST /api/chat` - Ask question (JSON: `{question: string, maxResults?: number}`)
- `GET /api/stats` - Collection statistics

## Development Notes

### Modifying the API

The API code is generated by `04-create-api-code.sh`. To modify:

1. Edit the heredoc sections in the script (search for `cat > src/...`)
2. Or manually edit files in `/opt/rag-system/api/src/`
3. Rebuild: `cd /opt/rag-system/api && npm run build`
4. Restart: `pm2 restart rag-api`

### Environment Variables

Located in `/opt/rag-system/api/.env`:
- `OLLAMA_URL` - Default: http://localhost:11434
- `QDRANT_URL` - Default: http://localhost:6333
- `COLLECTION_NAME` - Default: documents
- `MODEL_NAME` - Default: llama3.1:8b
- `PORT` - Default: 3000

### Text Chunking Strategy

Implemented in `rag.service.ts`:
- Default chunk size: 1000 characters
- Paragraph-aware splitting (preserves context at `\n\n` boundaries)
- Each chunk includes metadata: filename, chunkIndex, totalChunks

### Performance Characteristics

- First query: ~10 seconds (model warmup)
- Subsequent queries: ~2-4 seconds (simple), ~5-10 seconds (complex)
- Concurrent users: 1-2 (CPU-bound on EPYC 03)
- Embeddings: 384 dimensions
- Recommended PDFs: Text-based (OCR needed for scanned docs)

## Security Configuration

Firewall (UFW) allows ports:
- 22 (SSH)
- 80, 443 (HTTP/HTTPS)
- 3000 (API - adjust as needed)

Fail2ban configured for SSH protection.

For production: Add Nginx reverse proxy with SSL/TLS (Certbot).

## Troubleshooting

### API won't start
```bash
pm2 logs rag-api            # Check errors
pm2 restart rag-api         # Try restart
pm2 delete rag-api && pm2 start ecosystem.config.js  # Fresh start
```

### Qdrant connection failed
```bash
docker ps -a                # Check container status
docker logs qdrant          # Check errors
docker restart qdrant       # Restart
```

### Out of memory
- Check swap: `free -h`
- Monitor: `htop`
- Consider smaller model (8B instead of 13B)
- Add more swap if needed

### Slow responses
- Model too large for available RAM (switch to smaller model)
- Check CPU usage with `top`
- Verify swap isn't thrashing
- First query is always slower (model warmup)

## Testing

Run complete system test: `/root/05-test-system.sh`

Manual tests:
```bash
# Health check
curl http://localhost:3000/api/health

# Upload test PDF
curl -X POST http://localhost:3000/api/upload -F "file=@test.pdf"

# Test chat
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"question":"Was ist KI?"}'

# Check stats
curl http://localhost:3000/api/stats
```

## Important Files

- `/root/rag-setup.log` - Complete installation log
- `/root/rag-info.txt` - System info summary
- `/root/README.md` - Full documentation (German)
- `/opt/rag-system/qdrant/` - **CRITICAL: Regular backups required**
